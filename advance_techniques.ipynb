{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1c95da",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Techniques\n",
    "\n",
    "Objective: To implement and benchmark non-tree-based models (SVR, Neural Networks) and advanced ensemble strategies (Voting, Stacking) on the ACME Reimbursement dataset. We will assess if these complex architectures can capture the \"fuzzy\" logic better than Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13376f",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "SVR and Neural Networks require specific preprocessing (scaling) that decision trees do not. We import StandardScaler here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063d0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML Models\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Tree models for Ensembles\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# Data Preprocessing & Evaluation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set styles\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8e1e9",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Load the public_cases_derived_features.csv file containing the 27 engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53bef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 records.\n",
      "Data prepared: Scaled (for SVR/MLP) and Unscaled (for Trees).\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"public_cases_derived_features.csv\")\n",
    "    print(f\"Loaded {len(df)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'public_cases_derived_features.csv' not found.\")\n",
    "\n",
    "FEATURES = [col for col in df.columns if col != 'expected_output']\n",
    "X = df[FEATURES]\n",
    "y = df['expected_output']\n",
    "\n",
    "# --- CRITICAL STEP: Feature Scaling ---\n",
    "# SVR and Neural Networks perform poorly if data isn't scaled (e.g., miles vs. days).\n",
    "# We create a SCALED dataset for them.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split Scaled Data (for SVR/MLP)\n",
    "X_train_sc, X_val_sc, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.25, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Split Unscaled Data (for Tree Ensembles)\n",
    "X_train, X_val, _, _ = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Data prepared: Scaled (for SVR/MLP) and Unscaled (for Trees).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687dd62f",
   "metadata": {},
   "source": [
    "### Evaluation Helper\n",
    "\n",
    "The standard evaluation function for the challenge metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3f8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    diffs = np.abs(y_pred - y_true)\n",
    "    \n",
    "    exact = int(np.sum(diffs < 0.01))\n",
    "    close = int(np.sum(diffs < 1.00))\n",
    "    mae = np.mean(diffs)\n",
    "    score = (mae * 100) + (len(y_true) - exact) * 0.1\n",
    "    \n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"[{model_name}] Total={len(y_true)} | Exact={exact} | Close(< $1.00)={close} | MAE=${mae:.2f} | Score={score:.2f}\\n\")\n",
    "    \n",
    "    return {\"Model\": model_name, \"MAE\": mae, \"Score\": score, \"Exact\": exact}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39803d5",
   "metadata": {},
   "source": [
    "## Support Vector Regression (SVR)\n",
    "\n",
    "SVR tries to find a \"hyperplane\" in high-dimensional space that fits the data. It is powerful but computationally expensive and highly sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb4a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVR...\n",
      "--- Support Vector Regression ---\n",
      "[Support Vector Regression] Total=250 | Exact=0 | Close(< $1.00)=1 | MAE=$82.36 | Score=8261.15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize SVR\n",
    "# Kernel='rbf' allows it to learn non-linear curves (like the mileage tiering)\n",
    "# C=100 gives the model more flexibility to fit the complex data\n",
    "svr_model = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
    "\n",
    "# 2. Fit on SCALED data\n",
    "print(\"Training SVR...\")\n",
    "svr_model.fit(X_train_sc, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_svr = svr_model.predict(X_val_sc)\n",
    "\n",
    "# 4. Evaluate\n",
    "stats_svr = evaluate_model(y_val, y_pred_svr, \"Support Vector Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b008e9",
   "metadata": {},
   "source": [
    "## Neural Networks (MLP)\n",
    "\n",
    "We used a Multi-Layer Perceptron. Given the dataset size (1,000 rows), a massive deep network would overfit. We used a modest architecture (2 hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29d984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network...\n",
      "--- Neural Network (MLP) ---\n",
      "[Neural Network (MLP)] Total=250 | Exact=0 | Close(< $1.00)=4 | MAE=$82.86 | Score=8310.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize MLP\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64),  # Two layers\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=2000,     # High iteration count to ensure convergence\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=True, # Stop if validation score stops improving\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2. Fit on SCALED data\n",
    "print(\"Training Neural Network...\")\n",
    "mlp_model.fit(X_train_sc, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_mlp = mlp_model.predict(X_val_sc)\n",
    "\n",
    "# 4. Evaluate\n",
    "stats_mlp = evaluate_model(y_val, y_pred_mlp, \"Neural Network (MLP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb6264",
   "metadata": {},
   "source": [
    "## Ensemble Methods (Stacking & Voting)\n",
    "\n",
    "Here we combine the strengths of the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbdc808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base estimators defined for ensembles.\n"
     ]
    }
   ],
   "source": [
    "# Define fresh models for the ensemble. \n",
    "# We use the optimal n_estimators found in previous notebooks to avoid needing a validation set here.\n",
    "\n",
    "# XGBoost Base (approx optimal iterations)\n",
    "xgb_base = XGBRegressor(\n",
    "    n_estimators=800,  \n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# LightGBM Base (approx optimal iterations)\n",
    "lgbm_base = lgbm.LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(\"Base estimators defined for ensembles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25a2e8",
   "metadata": {},
   "source": [
    "## Voting Regressor\n",
    "\n",
    "A Voting Regressor averages the predictions of its base models. It often smooths out the \"jitter\" of individual tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5057a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Voting Regressor...\n",
      "--- Ensemble (Voting) ---\n",
      "[Ensemble (Voting)] Total=250 | Exact=0 | Close(< $1.00)=3 | MAE=$63.45 | Score=6370.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Voting Regressor\n",
    "voting_model = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_base), \n",
    "        ('lgbm', lgbm_base)\n",
    "    ],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Fit on UNSCALED data (Trees handle unscaled data better)\n",
    "print(\"Training Voting Regressor...\")\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_vote = voting_model.predict(X_val)\n",
    "\n",
    "# 4. Evaluate\n",
    "stats_vote = evaluate_model(y_val, y_pred_vote, \"Ensemble (Voting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f0950",
   "metadata": {},
   "source": [
    "## Stacking Regressor\n",
    "\n",
    "Stacking is more sophisticated. It trains a \"meta-model\" (Linear Regression) to learn the best combination of the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62002731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stacking Regressor...\n",
      "--- Ensemble (Stacking) ---\n",
      "[Ensemble (Stacking)] Total=250 | Exact=0 | Close(< $1.00)=3 | MAE=$58.90 | Score=5915.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Stacking Regressor\n",
    "# The 'final_estimator' learns how to weight the XGB and LGBM predictions\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_base), \n",
    "        ('lgbm', lgbm_base)\n",
    "    ],\n",
    "    final_estimator=LinearRegression(),\n",
    "    passthrough=False, # The meta-model only sees the predictions, not original features\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Fit on UNSCALED data\n",
    "print(\"Training Stacking Regressor...\")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict\n",
    "y_pred_stack = stacking_model.predict(X_val)\n",
    "\n",
    "# 4. Evaluate\n",
    "stats_stack = evaluate_model(y_val, y_pred_stack, \"Ensemble (Stacking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b6aa0",
   "metadata": {},
   "source": [
    "## Final Comparison\n",
    "\n",
    "Compare the advanced techniques against each other to see which architecture is the strongest candidate for the final Hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78443526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Advanced Technique Comparison ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c66d5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c66d5_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_c66d5_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "      <th id=\"T_c66d5_level0_col2\" class=\"col_heading level0 col2\" >Exact</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c66d5_level0_row0\" class=\"row_heading level0 row0\" >Ensemble (Stacking)</th>\n",
       "      <td id=\"T_c66d5_row0_col0\" class=\"data row0 col0\" >$58.90</td>\n",
       "      <td id=\"T_c66d5_row0_col1\" class=\"data row0 col1\" >5915.02</td>\n",
       "      <td id=\"T_c66d5_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c66d5_level0_row1\" class=\"row_heading level0 row1\" >Ensemble (Voting)</th>\n",
       "      <td id=\"T_c66d5_row1_col0\" class=\"data row1 col0\" >$63.45</td>\n",
       "      <td id=\"T_c66d5_row1_col1\" class=\"data row1 col1\" >6370.21</td>\n",
       "      <td id=\"T_c66d5_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c66d5_level0_row2\" class=\"row_heading level0 row2\" >Support Vector Regression</th>\n",
       "      <td id=\"T_c66d5_row2_col0\" class=\"data row2 col0\" >$82.36</td>\n",
       "      <td id=\"T_c66d5_row2_col1\" class=\"data row2 col1\" >8261.15</td>\n",
       "      <td id=\"T_c66d5_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c66d5_level0_row3\" class=\"row_heading level0 row3\" >Neural Network (MLP)</th>\n",
       "      <td id=\"T_c66d5_row3_col0\" class=\"data row3 col0\" >$82.86</td>\n",
       "      <td id=\"T_c66d5_row3_col1\" class=\"data row3 col1\" >8310.70</td>\n",
       "      <td id=\"T_c66d5_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16c4f18b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate results\n",
    "results = [stats_svr, stats_mlp, stats_vote, stats_stack]\n",
    "df_results = pd.DataFrame(results).set_index(\"Model\").sort_values(\"Score\")\n",
    "\n",
    "print(\"--- Advanced Technique Comparison ---\")\n",
    "display(df_results.style.format({'MAE': '${:.2f}', 'Score': '{:.2f}'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
